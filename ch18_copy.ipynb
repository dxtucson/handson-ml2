{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 18 – Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code in chapter 18_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. CNNs can be very slow without a GPU.\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    !apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
    "    !pip install -q -U tf-agents-nightly pyvirtualdisplay gym[atari]\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "else:\n",
    "    print(\"GPU was detected. Training will be faster.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"rl\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to OpenAI gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be using [OpenAI gym](https://gym.openai.com/), a great toolkit for developing and comparing Reinforcement Learning algorithms. It provides many environments for your learning *agents* to interact with. Let's start by importing `gym`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list all the available environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the system is unstable and after just a few wobbles, the pole ends up too tilted: game over. We will need to be smarter than that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TF-Agents to Beat Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use TF-Agents to create an agent that will learn to play Breakout. We will use the Deep Q-Learning algorithm, so you can easily compare the components with the previous implementation, but TF-Agents implements many other (and more sophisticated) algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-Agents Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.environments.wrappers.TimeLimit at 0x7ff29951c9d0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.environments import suite_gym\n",
    "\n",
    "env = suite_gym.load(\"Breakout-v4\")\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gym.envs.atari.atari_env.AtariEnv at 0x7ff299762f10>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.seed(42)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1) # Fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure breakout_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = env.render(mode=\"rgb_array\")\n",
    "\n",
    "plt.figure(figsize=(6, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "save_fig(\"breakout_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.current_time_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(210, 160, 3), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'), reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'), discount=BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0), observation=BoundedArraySpec(shape=(210, 160, 3), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.time_step_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can wrap a TF-Agents environments in a TF-Agents wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.environments.wrappers.ActionRepeat at 0x7ff299843f10>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.environments.wrappers import ActionRepeat\n",
    "\n",
    "repeating_env = ActionRepeat(env, times=4)\n",
    "repeating_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gym.envs.atari.atari_env.AtariEnv at 0x7ff299762f10>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeating_env.unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the list of available wrappers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActionClipWrapper           Wraps an environment and clips actions to spec before applying.\n",
      "ActionDiscretizeWrapper     Wraps an environment with continuous actions and discretizes them.\n",
      "ActionOffsetWrapper         Offsets actions to be zero-based.\n",
      "ActionRepeat                Repeates actions over n-steps while acummulating the received reward.\n",
      "FlattenObservationsWrapper  Wraps an environment and flattens nested multi-dimensional observations.\n",
      "GoalReplayEnvWrapper        Adds a goal to the observation, used for HER (Hindsight Experience Replay).\n",
      "HistoryWrapper              Adds observation and action history to the environment's observations.\n",
      "OneHotActionWrapper         Converts discrete action to one_hot format.\n",
      "PerformanceProfiler         End episodes after specified number of steps.\n",
      "PyEnvironmentBaseWrapper    PyEnvironment wrapper forwards calls to the given environment.\n",
      "RunStats                    Wrapper that accumulates run statistics as the environment iterates.\n",
      "TimeLimit                   End episodes after specified number of steps.\n"
     ]
    }
   ],
   "source": [
    "import tf_agents.environments.wrappers\n",
    "\n",
    "for name in dir(tf_agents.environments.wrappers):\n",
    "    obj = getattr(tf_agents.environments.wrappers, name)\n",
    "    if hasattr(obj, \"__base__\") and issubclass(obj, tf_agents.environments.wrappers.PyEnvironmentBaseWrapper):\n",
    "        print(\"{:27s} {}\".format(name, obj.__doc__.split(\"\\n\")[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `suite_gym.load()` function can create an env and wrap it for you, both with TF-Agents environment wrappers and Gym environment wrappers (the latter are applied first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "limited_repeating_env = suite_gym.load(\n",
    "    \"Breakout-v4\",\n",
    "    gym_env_wrappers=[partial(TimeLimit, max_episode_steps=10000)],\n",
    "    env_wrappers=[partial(ActionRepeat, times=4)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.environments.wrappers.ActionRepeat at 0x7ff29984bd90>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limited_repeating_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Atari Breakout environment, and wrap it to apply the default Atari preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gym.envs.atari.atari_env.AtariEnv at 0x7ff29983be90>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limited_repeating_env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import suite_atari\n",
    "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
    "from tf_agents.environments.atari_wrappers import FrameStack4\n",
    "\n",
    "max_episode_steps = 27000 # <=> 108k ALE frames since 1 step = 4 frames\n",
    "environment_name = \"BreakoutNoFrameskip-v4\"\n",
    "\n",
    "env = suite_atari.load(\n",
    "    environment_name,\n",
    "    max_episode_steps=max_episode_steps,\n",
    "    gym_env_wrappers=[AtariPreprocessing, FrameStack4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.environments.atari_wrappers.AtariTimeLimit at 0x7ff299851e50>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play a few steps just to see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "env.reset()\n",
    "time_step = env.step(1) # FIRE\n",
    "for _ in range(4):\n",
    "    time_step = env.step(3) # LEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_observation(obs):\n",
    "    # Since there are only 3 color channels, you cannot display 4 frames\n",
    "    # with one primary color per frame. So this code computes the delta between\n",
    "    # the current frame and the mean of the other frames, and it adds this delta\n",
    "    # to the red and blue channels to get a pink color for the current frame.\n",
    "    obs = obs.astype(np.float32)\n",
    "    img = obs[..., :3]\n",
    "    current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n",
    "    img[..., 0] += current_frame_delta\n",
    "    img[..., 2] += current_frame_delta\n",
    "    img = np.clip(img / 150, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure preprocessed_breakout_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plot_observation(time_step.observation)\n",
    "save_fig(\"preprocessed_breakout_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the Python environment to a TF environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "\n",
    "tf_env = TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small class to normalize the observations. Images are stored using bytes from 0 to 255 to use less RAM, but we want to pass floats from 0.0 to 1.0 to the neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Q-Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks.q_network import QNetwork\n",
    "\n",
    "preprocessing_layer = keras.layers.Lambda(\n",
    "                          lambda obs: tf.cast(obs, np.float32) / 255.)\n",
    "conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
    "fc_layer_params=[512]\n",
    "\n",
    "q_net = QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    preprocessing_layers=preprocessing_layer,\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the DQN Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "\n",
    "# see TF-agents issue #113\n",
    "#optimizer = keras.optimizers.RMSprop(lr=2.5e-4, rho=0.95, momentum=0.0,\n",
    "#                                     epsilon=0.00001, centered=True)\n",
    "\n",
    "train_step = tf.Variable(0)\n",
    "update_period = 4 # run a training step every 4 collect steps\n",
    "optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate=2.5e-4, decay=0.95, momentum=0.0,\n",
    "                                     epsilon=0.00001, centered=True)\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0, # initial ε\n",
    "    decay_steps=250000 // update_period, # <=> 1,000,000 ALE frames\n",
    "    end_learning_rate=0.01) # final ε\n",
    "agent = DqnAgent(tf_env.time_step_spec(),\n",
    "                 tf_env.action_spec(),\n",
    "                 q_network=q_net,\n",
    "                 optimizer=optimizer,\n",
    "                 target_update_period=2000, # <=> 32,000 ALE frames\n",
    "                 td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n",
    "                 gamma=0.99, # discount factor\n",
    "                 train_step_counter=train_step,\n",
    "                 epsilon_greedy=lambda: epsilon_fn(train_step))\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=1000000)\n",
    "\n",
    "replay_buffer_observer = replay_buffer.add_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple custom observer that counts and displays the number of times it is called (except when it is passed a trajectory that represents the boundary between two episodes, as this does not count as a step):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowProgress:\n",
    "    def __init__(self, total):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some training metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.metrics import tf_metrics\n",
    "\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics[0].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 0\n",
      "\t\t EnvironmentSteps = 0\n",
      "\t\t AverageReturn = 0.0\n",
      "\t\t AverageEpisodeLength = 0.0\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "log_metrics(train_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the collect driver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "\n",
    "collect_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer_observer] + train_metrics,\n",
    "    num_steps=update_period) # collect 4 steps for each training iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the initial experiences, before training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000"
     ]
    }
   ],
   "source": [
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "\n",
    "initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n",
    "                                        tf_env.action_spec())\n",
    "init_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    initial_collect_policy,\n",
    "    observers=[replay_buffer.add_batch, ShowProgress(20000)],\n",
    "    num_steps=20000) # <=> 80,000 ALE frames\n",
    "final_time_step, final_policy_state = init_driver.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample 2 sub-episodes, with 3 time steps each and display them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(888) # chosen to show an example of trajectory at the end of an episode\n",
    "\n",
    "trajectories, buffer_info = replay_buffer.get_next(\n",
    "    sample_batch_size=2, num_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 84, 84, 4])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories.observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2, 84, 84, 4])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.trajectories.trajectory import to_transition\n",
    "\n",
    "time_steps, action_steps, next_time_steps = to_transition(trajectories)\n",
    "time_steps.observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 1]], dtype=int32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories.step_type.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure sub_episodes_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x489.6 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6.8))\n",
    "for row in range(2):\n",
    "    for col in range(3):\n",
    "        plt.subplot(2, 3, row * 3 + col + 1)\n",
    "        plot_observation(trajectories.observation[row, col].numpy())\n",
    "plt.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0, wspace=0.02)\n",
    "save_fig(\"sub_episodes_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2,\n",
    "    num_parallel_calls=3).prefetch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the main functions to TF Functions for better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.utils.common import function\n",
    "\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "agent.train = function(agent.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are ready to run the main loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(n_iterations):\n",
    "    time_step = None\n",
    "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
    "    iterator = iter(dataset)\n",
    "    for iteration in range(n_iterations):\n",
    "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "        trajectories, buffer_info = next(iterator)\n",
    "        train_loss = agent.train(trajectories)\n",
    "        print(\"\\r{} loss:{:.5f}\".format(\n",
    "            iteration, train_loss.loss.numpy()), end=\"\")\n",
    "        if iteration % 1000 == 0:\n",
    "            log_metrics(train_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to train the agent for 10,000 steps. Then look at its behavior by running the following cell. You can run these two cells as many times as you wish. The agent will keep improving!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 0\n",
      "\t\t EnvironmentSteps = 4\n",
      "\t\t AverageReturn = 0.0\n",
      "\t\t AverageEpisodeLength = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998 loss:0.00003"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 24\n",
      "\t\t EnvironmentSteps = 4004\n",
      "\t\t AverageReturn = 0.8999999761581421\n",
      "\t\t AverageEpisodeLength = 169.39999389648438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1476 loss:0.00001"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-d8cdd578cf9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-89-2e7ba9c097bc>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(n_iterations)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtrajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         print(\"\\r{} loss:{:.5f}\".format(\n\u001b[1;32m     10\u001b[0m             iteration, train_loss.loss.numpy()), end=\"\")\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_agent(n_iterations=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "def save_frames(trajectory):\n",
    "    global frames\n",
    "    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n",
    "\n",
    "prev_lives = tf_env.pyenv.envs[0].ale.lives()\n",
    "def reset_and_fire_on_life_lost(trajectory):\n",
    "    global prev_lives\n",
    "    lives = tf_env.pyenv.envs[0].ale.lives()\n",
    "    if prev_lives != lives:\n",
    "        tf_env.reset()\n",
    "        tf_env.pyenv.envs[0].step(1)\n",
    "        prev_lives = lives\n",
    "\n",
    "watch_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.policy,\n",
    "    observers=[save_frames, reset_and_fire_on_life_lost, ShowProgress(1000)],\n",
    "    num_steps=1000)\n",
    "final_time_step, final_policy_state = watch_driver.run()\n",
    "\n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to save an animated GIF to show off your agent to your friends, here's one way to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "image_path = os.path.join(\"images\", \"rl\", \"breakout.gif\")\n",
    "frame_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\n",
    "frame_images[0].save(image_path, format='GIF',\n",
    "                     append_images=frame_images[1:],\n",
    "                     save_all=True,\n",
    "                     duration=30,\n",
    "                     loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"images/rl/breakout.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
